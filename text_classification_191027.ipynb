{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:07, 16887.29lines/s]\n",
      "120000lines [00:12, 9675.63lines/s] \n",
      "7600lines [00:00, 9813.16lines/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "NGRAMS = 2\n",
    "import os\n",
    "if not os.path.isdir('./.data'):\n",
    "\tos.mkdir('./.data')\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
    "##train_dataset, test_dataset = torchtext.datasets.AG_NEWS(ngrams=3) 이런방식으로 로딩 가능\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3  Wall St. Bears Claw Back Into the Black (Reuters)  \\\n",
      "0   3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
      "1   3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
      "2   3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
      "3   3  Oil prices soar to all-time record, posing new...   \n",
      "4   3        Stocks End Up, But Near Year Lows (Reuters)   \n",
      "5   3               Money Funds Fell in Latest Week (AP)   \n",
      "6   3  Fed minutes show dissent over inflation (USATO...   \n",
      "7   3                            Safety Net (Forbes.com)   \n",
      "8   3            Wall St. Bears Claw Back Into the Black   \n",
      "9   3              Oil and Economy Cloud Stocks' Outlook   \n",
      "10  3             No Need for OPEC to Pump More-Iran Gov   \n",
      "11  3          Non-OPEC Nations Should Up Output-Purnomo   \n",
      "12  3              Google IPO Auction Off to Rocky Start   \n",
      "13  3           Dollar Falls Broadly on Record Trade Gap   \n",
      "14  3                              Rescuing an Old Saver   \n",
      "15  3                       Kids Rule for Back-to-School   \n",
      "16  3          In a Down Market, Head Toward Value Funds   \n",
      "17  3                    US trade deficit swells in June   \n",
      "18  3                  Shell 'could be target for Total'   \n",
      "19  3                   Google IPO faces Playboy slip-up   \n",
      "\n",
      "   Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.  \n",
      "0   Reuters - Private investment firm Carlyle Grou...                                              \n",
      "1   Reuters - Soaring crude prices plus worries\\ab...                                              \n",
      "2   Reuters - Authorities have halted oil export\\f...                                              \n",
      "3   AFP - Tearaway world oil prices, toppling reco...                                              \n",
      "4   Reuters - Stocks ended slightly higher on Frid...                                              \n",
      "5   AP - Assets of the nation's retail money marke...                                              \n",
      "6   USATODAY.com - Retail sales bounced back a bit...                                              \n",
      "7   Forbes.com - After earning a PH.D. in Sociolog...                                              \n",
      "8    NEW YORK (Reuters) - Short-sellers, Wall Stre...                                              \n",
      "9    NEW YORK (Reuters) - Soaring crude prices plu...                                              \n",
      "10   TEHRAN (Reuters) - OPEC can do nothing to dou...                                              \n",
      "11   JAKARTA (Reuters) - Non-OPEC oil exporters sh...                                              \n",
      "12   WASHINGTON/NEW YORK (Reuters) - The auction f...                                              \n",
      "13   NEW YORK (Reuters) - The dollar tumbled broad...                                              \n",
      "14  If you think you may need to help your elderly...                                              \n",
      "15  The purchasing power of kids is a big part of ...                                              \n",
      "16  There is little cause for celebration in the s...                                              \n",
      "17  The US trade deficit has exploded 19 to a reco...                                              \n",
      "18  Oil giant Shell could be bracing itself for a ...                                              \n",
      "19  The bidding gets underway for Google's public ...                                              \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "dataset example\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./.data/ag_news_csv/train.csv', nrows=20)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "EmbeddingBag params\n",
    "vocab_size = num_embeddings (int) – size of the dictionary of embeddings\n",
    "embed_dim = embedding_dim (int) – the size of each embedding vector\n",
    "sparse (bool, optional) – if True, gradient w.r.t. weight matrix will be a sparse tensor.\n",
    "''' \n",
    "\n",
    "\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "'''\n",
    "AG_NEWS classes\n",
    "1:world\n",
    "2:sports\n",
    "3:business\n",
    "4:sci/tec\n",
    "'''\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    \n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text) # purpose of torch.cat() is concat... why torch.cat is used???\n",
    "    \n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "    \n",
    "    #Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                     collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "    \n",
    "    #growing lr\n",
    "    scheduler.step()\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "            \n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 31 seconds\n",
      "\tLoss: 0.0261(train)\t|\tAcc: 84.9%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 89.2%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 33 seconds\n",
      "\tLoss: 0.0119(train)\t|\tAcc: 93.6%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.3%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 33 seconds\n",
      "\tLoss: 0.0069(train)\t|\tAcc: 96.4%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 90.6%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 33 seconds\n",
      "\tLoss: 0.0039(train)\t|\tAcc: 98.1%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 91.5%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 34 seconds\n",
      "\tLoss: 0.0022(train)\t|\tAcc: 99.0%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.4%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0003(test)\t|\tAcc: 88.1%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        print(text)\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "def printResult(plain_text):\n",
    "    print(\"predicted result is : \" + ag_news_label[predict(plain_text, model, vocab, 2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   1330,    4949,      19,       3,    9145,     103,      83,  134201,\n",
      "         269893,       6,    8779,       4,     156,      19,     797,       2,\n",
      "           2774,       2,    3328,       2,    1411,    3966,       0,      22,\n",
      "           1171,       0,     950,     301,       3,     236,     554,    6324,\n",
      "              9,      61,     317,       2,       3,    2125,      24,       3,\n",
      "          20724,       7,  107758,       0,     178,      91,       2,   75032,\n",
      "         113948,     250,   76765,  405587,    7617,       0,       0, 1053572,\n",
      "          54490,  336646,    1323,  158895,       0,    1571,    6736,    4269,\n",
      "          12655,    5845,   61793,       0,       0,       0,   40333,       0,\n",
      "              0,  280411,     995,    4184,    6057,       0,  274506,   29957,\n",
      "           1930,    3303,     105,   23939,  269740,     829,   70756,  266015,\n",
      "              0,       0,       0,   16407,   28964])\n",
      "predicted result is : Sports\n"
     ]
    }
   ],
   "source": [
    "plain_text1 = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "plain_text2 = \"LeCun once told Wired that deep learning is really a conspiracy between\\\n",
    "    Geoff Hinton and myself and Yoshua Bengio, from the University of\\\n",
    "    Montreal. While Hinton works on AI at Google, and Bengio splits time\\\n",
    "    between University of Montreal and data mining company ApStat, LeCun\\\n",
    "    has been able to snag other top-shelf names.\"\n",
    "plain_text3 = \"Never mind that the Nationals were not prematurely prepping a\\\n",
    "    celebration, or that M.L.B. does similar run-throughs at each team’s stadium\\\n",
    "    during the league championship rounds and World Series. The practice is the \\\n",
    "    opposite of arrogance; it’s all about.\"\n",
    "plain_text4 = \"Never mind that the Nationals were not prematurely prepping a celebration, or that M.L.B. does similar run-throughs at each team’s stadium during the league championship rounds and World Series. The practice is the opposite of arrogance; it’s all about worrying what can go wrong if the first stage setup happens on live television.\\\n",
    "    Still, the move has been second-guessed almost as much as any pitching choice or umpire’s call. Perhaps that is not surprising in a sport that dwells on superstitions more than any other, turning unfounded rituals into staples of the game.\\\n",
    "    Broadcasters sometimes refuse to acknowledge a no-hitter in progress, and teammates often don’t talk to a pitcher working on one. Many pitchers eat the same food before each game — or consume unusual concoctions, such as banana-mayonnaise sandwiches — to conjure a victory.\\\n",
    "    The teams in this World Series have their share of adherents to such traditions. Nationals General Manager Mike Rizzo donned the same red sweatshirt for each of the eight games in his team’s postseason winning streak, and Astros third baseman Alex Bregman wore the same plaid shirt to the ballpark during his team’s three-game World Series winning streak.\"\n",
    "\n",
    "printResult(plain_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e0211cbce30a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/3d/4e983cd98d87b50b2ab0387d73fa946f745aa8164e8888a714d5129f9765/konlpy-0.5.1-py2.py3-none-any.whl (19.4MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4MB 457kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting JPype1>=0.5.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/63/784834e8a24ec2e1ad7f703c3dc6c6fb372a77cc68a2fdff916e18a4449e/JPype1-0.7.0.tar.gz (470kB)\n",
      "\u001b[K     |████████████████████████████████| 471kB 408kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: JPype1\n",
      "  Building wheel for JPype1 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for JPype1: filename=JPype1-0.7.0-cp37-cp37m-macosx_10_9_x86_64.whl size=244175 sha256=7a631af331e745b9cadc987a69a1ce04765d43dc4a2387d914105f06d52e8a3e\n",
      "  Stored in directory: /Users/kihunum/Library/Caches/pip/wheels/68/68/4f/c5f2d175cb26a2765561069a80c4285488d17be01eecb21597\n",
      "Successfully built JPype1\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-0.7.0 konlpy-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
